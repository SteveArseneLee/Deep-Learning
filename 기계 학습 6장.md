# 지도 학습과 비지도 학습, 준지도 학습
- 지도 학습 : 모든 훈련 샘플이 레이블 정보를 가짐
- 비지도 학습 : 모든 훈련 샘플이 레이블 정보를 가지지 않음
- 준지도 학습 : 레이블을 가진 샘플과 가지지 않은 샘플이 섞임

### 기계 학습이 사용하는 두 종류의 지식
- 훈련 집합
- 사전 지식

비지도 학습과 준지도 학습은 사전 지식을 더 명시적으로 사용

### 비지도 학습의 세 가지 일반 과업
- 군집화 : 유사한 샘플을 모아 같은 그룹으로 묶는 일
- 밀도 추정 : 데이터로부터 확률분포를 추정하는 일
- 공간 변환 : 원래 특징 공간을 저차원 또는 고차원 공간으로 변환하는 일

<군집화>
군집화 문제
- X = {x1, x2, ..., xn}에서 군집집합 C = {c1, c2, c3, ..., ck}를 찾아내는 작업
- 군집의 개수 k는 주어지는 경우와 자동으로 찾아야 하는 경우가 있음
- 군집화를 부류 발견 작업이라 부르기도 함

k-평균 알고리즘의 특성
- 군집 개수(k)와 군집 중심의 초기 위치(z1, z2, ...,zk)가 주어질 때, 각 샘플 별로 가까운 군집으로 할당 -> 군집 중심의 위치는 군집의 할당된 샘플 평균으로 갱신
- 원리는 단순하지만 성능이 좋아 인기가 좋음, 직관적으로 이해하기 쉽고 구현 쉬움, 군집 개수 k를 알려줘야 함

k-평균과 k-mediods
- k-평균은 샘플의 평균으로 군집 중심을 갱신
- k-mediods는 대표를 뽑아 뽑힌 대표로 군집 중심을 갱신(k-평균에 비해 잡음에 둔감)
최적화 문제로 해석
- k-평균은 목적함수를 최소화하는 알고리즘
- 행렬 A는 군집 배정 정보를 나타내는 k*n행렬(i번째 샘플이 j번째 군집에 배정되었다면 aji는 1, 그렇지 않으면 0)

### EM(Expectation Maximizatoin)
- k-평균에서 훈련집합 X와 군집집합 C(행렬 A)는 각각 입력단과 출력단에서 관찰 가능
- 중간 단계의 입시 변수 Z(입출력단에서 보이지 않기 때문에 은닉변수라 부름)
- k-평균은 Z의 추정(E 단계)과 A의 추정(M 단계)을 번갈아 가면 수행하는 EM 알고리즘

히스토그램 방법
- 특징 공간을 칸의 집합으로 분할한 다음, 칸에 있는 샘플의 빈도를 세어 추정함
- 여러 문제점
    - 매끄럽지 못하고 계단 모야을 띠는 확률밀도함수가 됨
    - 칸의 크기와 위치에 민감함

커널 밀도 추정법
- 커널(kernel) : n차원공간을 n차원 공간으로 사상하는 함수
- 점 x에 예시하는 커널을 씌우고 커널 안에 있는 샘플의 가중 합을 이용함
- 대역폭 h의 크기가 중요

히스토그램 방법과 커널 밀도 추정법의 비교
- 커널 밀도 추정법은 매끄러운 확률밀도함수를 추정

커널 밀도 추정법에서 대역폭 h의 중요성
- h가 너무 작으면 뾰족뾰족한 모양
- h가 너무 크면 뭉개짐
-> 적절하게 설정해야 함

커널 밀도 추정 기법의 근본적 문제점
- 샘플을 모두 저장하고 있어야하는 메모리 기반 방법
- 데이터 희소성(차원의 저주) -> 데이터가 낮은 차원인 경우로 국한하여 활용

가우시안을 이용한 방법
- 데이터가 가우시안 분포를 따른다고 가정하고 평균벡터와 공분산 행렬을 추정